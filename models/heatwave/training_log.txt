============================================================
EXPERIMENT 3: HEATWAVE PREDICTION (Classification)
  v2: Added XGBoost + SMOTE + new features (diurnal, precip, heat streak)
============================================================

[Data] Loading and splitting data...
   Train shape: (3208, 22), Test shape: (803, 22)
   Features: ['temp_max_lag1', 'temp_max_lag2', 'temp_max_lag3', 'temp_max_7day_avg', 'humidity', 'month_sin', 'month_cos', 'month', 'diurnal_range_lag1', 'temp_min_lag1', 'temp_min_7day_avg', 'precip_7day_sum', 'heat_streak', 'wind_speed_lag1', 'wind_gusts_lag1', 'solar_rad_lag1', 'solar_rad_3day_avg', 'et0_lag1', 'et0_7day_avg', 'soil_moisture_lag1', 'pressure_lag1', 'pressure_change_1d']
   Heatwave Ratio (before SMOTE): 2.04%
   Class distribution - 0: 3929, 1: 82

[SMOTE] Oversampling minority class (heatwave)...
   After SMOTE - Train shape: (4713, 22)
   After SMOTE - Class 0: 3142, Class 1: 1571

[Experiment] Training candidate models on SMOTE-resampled data...
   > Training Logistic Regression...
     Best Params: {'classifier__C': 1.0}
     Accuracy: 0.9290 | F1: 0.2785
     Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.93      0.96       787
    Heatwave       0.17      0.69      0.28        16

    accuracy                           0.93       803
   macro avg       0.58      0.81      0.62       803
weighted avg       0.98      0.93      0.95       803

   > Training Random Forest...
     Best Params: {'max_depth': None, 'n_estimators': 100}
     Accuracy: 0.9826 | F1: 0.5625
     Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.99      0.99       787
    Heatwave       0.56      0.56      0.56        16

    accuracy                           0.98       803
   macro avg       0.78      0.78      0.78       803
weighted avg       0.98      0.98      0.98       803

   > Training Gradient Boosting...
     Best Params: {'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 100}
     Accuracy: 0.9801 | F1: 0.5294
     Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.99      0.99       787
    Heatwave       0.50      0.56      0.53        16

    accuracy                           0.98       803
   macro avg       0.75      0.78      0.76       803
weighted avg       0.98      0.98      0.98       803

   > Training XGBoost...
     Best Params: {'colsample_bytree': 0.8, 'learning_rate': 0.1, 'max_depth': 7, 'n_estimators': 300, 'reg_alpha': 0.1, 'reg_lambda': 1, 'scale_pos_weight': 1, 'subsample': 0.8}
     Accuracy: 0.9851 | F1: 0.5714
     Classification Report:
              precision    recall  f1-score   support

      Normal       0.99      0.99      0.99       787
    Heatwave       0.67      0.50      0.57        16

    accuracy                           0.99       803
   macro avg       0.83      0.75      0.78       803
weighted avg       0.98      0.99      0.98       803


============================================================
WINNER: XGBoost (F1 Score: 0.5714)
============================================================
Feature importance plot saved.
Plots saved to C:\Users\prabina\Code\resilience-grid-main\.claude\worktrees\trusting-wu\static\plots
Report saved to C:\Users\prabina\Code\resilience-grid-main\.claude\worktrees\trusting-wu\models\heatwave
